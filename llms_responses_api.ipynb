{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openrouter_client = AsyncOpenAI(\n",
    "    base_url=f\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"\"  # api-ключ,\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "glama_client = AsyncOpenAI(\n",
    "    base_url=\"https://glama.ai/api/gateway/openai/v1\",\n",
    "    api_key=\"\"  # api-ключ\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "models_key = ['gemini-2.0', 'llama-3.3', 'qwen-2.5', 'mistral-nemo',\n",
    "              'moonlight-16b-a3b', 'gemma-3-27b', 'deephermes-3-8b', 'ds-distill-llama']\n",
    "models = ['gemini-2.0-flash-thinking-exp-01-21',\n",
    "          'meta-llama/llama-3.3-70b-instruct:free',\n",
    "          'qwen/qwen-2.5-72b-instruct:free',\n",
    "          'mistralai/mistral-nemo:free',\n",
    "          'moonshotai/moonlight-16b-a3b-instruct:free',\n",
    "          'google/gemma-3-27b-it:free',\n",
    "          'nousresearch/deephermes-3-llama-3-8b-preview:free',\n",
    "          'deepseek/deepseek-r1-distill-llama-70b:free']\n",
    "models_dict = dict(zip(models_key, models))\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, use_cache=True, base_model=None, local_client=openrouter_client):\n",
    "        self.reset(use_cache, base_model)\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.local_client = local_client\n",
    "    \n",
    "    def reset(self, use_cache=True, base_model=None):\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "        self.base_model = base_model\n",
    "\n",
    "    async def _chat_base(self, messages, temperature=0, top_p=1, max_tokens=4096): \n",
    "        if isinstance(messages, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "            \n",
    "        model = models_dict.get(self.base_model, self.base_model)\n",
    "        cache_id = str(messages) + model\n",
    "        \n",
    "        if cache_id in self.cache and temperature == 0 and self.use_cache:\n",
    "            return deepcopy(self.cache[cache_id])\n",
    "            \n",
    "        try:\n",
    "            response = await self.local_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_tokens=max_tokens,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            self.cache[cache_id] = [choice.message.content for choice in response.choices]\n",
    "            return deepcopy(self.cache[cache_id])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def chat(self, messages, temperature=0, top_p=1, max_tokens=4096):\n",
    "        return await self._chat_base(messages, temperature, top_p, max_tokens)\n",
    "\n",
    "    async def chat_serial(self, messages, temperature=0, top_p=1, max_tokens=4096):\n",
    "        async with self.lock:\n",
    "            response = await self._chat_base(\n",
    "                messages,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "\n",
    "            return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = LLM(base_model='gemini-2.0', local_client=glama_client)\n",
    "llama_llm = LLM(base_model='llama-3.3')\n",
    "qwen_llm = LLM(base_model='qwen-2.5')\n",
    "mistral_llm = LLM(base_model='mistral-nemo')\n",
    "moonlight_llm = LLM(base_model='moonlight-16b-a3b')\n",
    "gemma_llm = LLM(base_model='gemma-3-27b')\n",
    "deephermes_llm = LLM(base_model='deephermes-3-8b')\n",
    "ds_distill_llama_llm = LLM(base_model='ds-distill-llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_llm(llm, handshake_prompt):\n",
    "    try:\n",
    "        response = await llm.chat(handshake_prompt)\n",
    "        if response:\n",
    "            print(f\"Model initialization response: {response[0]}\")\n",
    "            return response[0].lower().strip().rstrip('.') in [\"yes\", \"y\"]\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Initialization error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_original_df(excel_file, results_df, sheet_name):\n",
    "    book = load_workbook(excel_file)\n",
    "    if sheet_name in book.sheetnames:\n",
    "        del book[sheet_name]\n",
    "    \n",
    "    book.save(excel_file)\n",
    "    book.close()\n",
    "\n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a') as writer:\n",
    "        results_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    return pd.read_excel(excel_file, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 эксперимент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_excel = 'exp1_dataset.xlsx'\n",
    "exp1_df = pd.read_excel(exp1_excel, sheet_name = 'base sheet')\n",
    "all_items = sorted(exp1_df['Item'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDSHAKE_PROMPT_1 = \"\"\"\n",
    "Hi! I want you to help me with my experiment.\n",
    "I will send groups of English sentences. For each sentence, rate its naturalness from 1 (completely unnatural) to 7 (completely natural).\n",
    "\n",
    "RULES:\n",
    "1. Return ONLY a comma-separated list of numbers (e.g., \"5,3,6\")\n",
    "2. No explanations, no formatting, just numbers\n",
    "3. Maintain exact order of input sentences\n",
    "\n",
    "Do you understand the instructions? Answer strictly \"yes\" or \"no\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_exp1_dataset(df, llm):\n",
    "    results_df = pd.DataFrame(columns=['Item', 'Sentence', 'Rating'])\n",
    "\n",
    "    for item in tqdm(all_items, desc=\"Processing groups\"):\n",
    "        sentences = df[df['Item'] == item]['Sentence'].tolist()\n",
    "\n",
    "        prompt = \"\"\"Rate each sentence's naturalness (1-7). Return EXACTLY 6 numbers separated by commas, like: 5,3,6,2,4,7\n",
    "\n",
    "        Sentences:\n",
    "        \"\"\" + \"\\n\".join(f\"{i+1}. {s}\" for i,s in enumerate(sentences))\n",
    "\n",
    "        try:\n",
    "            response = await llm.chat_serial(prompt)\n",
    "            raw_response = response[0] if response else \"\"\n",
    "            print(f\"\\nRAW RESPONSE:\\n{raw_response}\\n\")\n",
    "\n",
    "            # парсинг\n",
    "            ratings = []\n",
    "            if raw_response:\n",
    "                parts = raw_response.replace(\" \", \"\").split(\",\")\n",
    "                for part in parts:\n",
    "                    try:\n",
    "                        num = int(part)\n",
    "                        ratings.append(num if 1 <= num <= 7 else None)\n",
    "                    except (ValueError, TypeError):\n",
    "                        ratings.append(None)\n",
    "\n",
    "            new_rows = []\n",
    "            for _, (sent, rating) in enumerate(zip(sentences, ratings)):\n",
    "                new_rows.append({\n",
    "                    'Item': item,\n",
    "                    'Sentence': sent,\n",
    "                    'Rating': rating\n",
    "                })\n",
    "            \n",
    "            if new_rows:\n",
    "                results_df = pd.concat([\n",
    "                    results_df, \n",
    "                    pd.DataFrame(new_rows)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in item {item}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not await initialize_llm(ds_distill_llama_llm, HANDSHAKE_PROMPT_1):\n",
    "    raise ValueError(\"Model initialization failed\")\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp1 = await process_exp1_dataset(exp1_df, ds_distill_llama_llm)\n",
    "display(results_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_original_df(exp1_excel, results_exp1, 'ds_distill_llama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 эксперимент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_excel = 'exp2_dataset.xlsx'\n",
    "exp2_df = pd.read_excel(exp2_excel, sheet_name = 'base sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDSHAKE_PROMPT_2 = \"\"\"\n",
    "Hi! I want you to participate in my experiment.\n",
    "\n",
    "TASK STRUCTURE:\n",
    "1. I will send you dialogues in the following format:\n",
    "   <Dialogue N>\n",
    "   Context: [4 context sentences]\n",
    "   Sentence to evaluate: [last sentence]\n",
    "   Questions:\n",
    "   1. [question 1]\n",
    "   2. [question 2]\n",
    "   </Dialogue N>\n",
    "\n",
    "2. For each dialogue, respond with answers for both question 1 and question 2.\n",
    "\n",
    "Do you understand the instructions? Answer strictly \"yes\" or \"no\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_exp2(batch, llm, results_df):\n",
    "    new_rows = []\n",
    "\n",
    "    for i, (_, row) in enumerate(batch.iterrows()):\n",
    "        context = f\"Context: {row['sentence1']} {row['sentence2']} {row['sentence3']} {row['sentence4']}\"\n",
    "        response = row['sentence5']\n",
    "        question1 = f\"Question 1: {row['question1']}\"\n",
    "        question2 = f\"Question 2: {row['question2']}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            <Dialogue {i+1}>\n",
    "            {context}\n",
    "            {response}\n",
    "            Questions for you:\n",
    "            1. {question1}\n",
    "            Please provide the score only from 1 (completely unnatural) to 7 (completely natural). Do not send any comments or reasoning.\n",
    "            Next question:\n",
    "            2. {question2}\n",
    "            Please provide the score only from 1 (absolutely no) to 7 (absolutely yes). Do not send any comments or reasoning.\n",
    "            </Dialogue {i+1}>\n",
    "            \"\"\"\n",
    "\n",
    "        try:\n",
    "            llm_response = await llm.chat_serial(prompt)\n",
    "            raw_response = llm_response[0] if llm_response else \"\"\n",
    "            print(f\"\\nRAW RESPONSE for item {row['Item']}:\\n{raw_response}\\n\")\n",
    "\n",
    "            new_rows.append({\n",
    "                'Group': row['Group'],\n",
    "                'question1': row['question1'],\n",
    "                'Q1_answer': None,\n",
    "                'question2': row['question2'],\n",
    "                'Q2_answer': None,\n",
    "                'sentence1': row['sentence1'],\n",
    "                'sentence2': row['sentence2'],\n",
    "                'sentence3': row['sentence3'],\n",
    "                'sentence4': row['sentence4'],\n",
    "                'sentence5': row['sentence5'],\n",
    "                'Item': row['Item'],\n",
    "                'Condition': row['Condition'],\n",
    "                'raw response': raw_response\n",
    "                    })  \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {row['Item']}: {str(e)}\")\n",
    "        \n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_batches_exp2(df, llm):\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'Group',\n",
    "        'question1', 'Q1_answer', 'question2', 'Q2_answer',\n",
    "        'sentence1', 'sentence2', 'sentence3', 'sentence4',\n",
    "        'sentence5', 'Item', 'Condition', 'raw response'\n",
    "    ])\n",
    "\n",
    "    for i in tqdm(range(0, len(df), 3), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+3]\n",
    "        results_df = await process_batch_exp2(batch, llm, results_df)\n",
    "\n",
    "        await asyncio.sleep(random.randint(10, 20))\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not await initialize_llm(moonlight_llm, HANDSHAKE_PROMPT_2):\n",
    "    raise ValueError(\"Model initialization failed\")\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp2 = await process_all_batches_exp2(exp2_df, moonlight_llm)\n",
    "display(results_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_original_df(exp2_excel, results_exp2, 'moonlight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 эксперимент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_excel = 'exp3_dataset.xlsx'\n",
    "exp3_df = pd.read_excel(exp3_excel, sheet_name = 'base sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDSHAKE_PROMPT_3 = \"\"\"\n",
    "Hi! I want you to participate in my experiment.\n",
    "\n",
    "TASK STRUCTURE:\n",
    "1. I will send you dialogues in the following format:\n",
    "   <Dialogue N>\n",
    "   Context: [3 context sentences]\n",
    "   Sentence to evaluate: [last sentence]\n",
    "   Questions:\n",
    "   1. [question 1]\n",
    "   2. [question 2]\n",
    "   </Dialogue N>\n",
    "\n",
    "2. For each dialogue, respond with answers for both question 1 and question 2.\n",
    "\n",
    "Do you understand the instructions? Answer strictly \"yes\" or \"no\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_exp3(batch, llm, results_df):\n",
    "    new_rows = []\n",
    "\n",
    "    for i, (_, row) in enumerate(batch.iterrows()):\n",
    "        context = f\"Context: {row['S1']} {row['S2']} {row['S3']}\"\n",
    "        response = row['S4']\n",
    "        question1 = f\"Question 1: {row['Q1']}\"\n",
    "        question2 = f\"Question 2: {row['Q2']}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            <Dialogue {i+1}>\n",
    "            {context}\n",
    "            {response}\n",
    "            Questions for you:\n",
    "            1. {question1}\n",
    "            Please provide the score only from 1 (absolutely no) to 7 (absolutely yes) without any comments or reasoning\n",
    "            2. {question2}\n",
    "            Please provide the score only from 1 (absolutely no) to 7 (absolutely yes) without any comments or reasoning\n",
    "            </Dialogue {i+1}>\n",
    "            \"\"\"\n",
    "\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                llm_response = await llm.chat_serial(prompt)\n",
    "                raw_response = llm_response[0] if llm_response else \"\"\n",
    "                print(f\"\\nRAW RESPONSE for item {row['item']}:\\n{raw_response}\\n\")\n",
    "\n",
    "                new_rows.append({\n",
    "                    'item': row['item'],\n",
    "                    'condition': row['condition'],\n",
    "                    'Q1': row['Q1'],\n",
    "                    'Q1_answer': None,\n",
    "                    'Q2': row['Q2'],\n",
    "                    'Q2_answer': None,\n",
    "                    'raw response': raw_response,\n",
    "                    'S1': row['S1'],\n",
    "                    'S2': row['S2'],\n",
    "                    'S3': row['S3'],\n",
    "                    'S4': row['S4'],\n",
    "                })\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == 0:\n",
    "                    print(f\"Error on first attempt for item {row['item']}: {str(e)}. Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Second attempt failed for item {row['item']}. Skipping.\")\n",
    "\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_batches_exp3(df, llm):\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'item', 'condition',\n",
    "        'Q1', 'Q1_answer', 'Q2', 'Q2_answer',\n",
    "        'raw response', 'S1', 'S2', 'S3', 'S4'\n",
    "    ])\n",
    "\n",
    "    for i in tqdm(range(0, len(df), 4), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+4]\n",
    "        results_df = await process_batch_exp3(batch, llm, results_df)\n",
    "\n",
    "        await asyncio.sleep(random.randint(10, 20))\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not await initialize_llm(gemini_llm, HANDSHAKE_PROMPT_3):\n",
    "    raise ValueError(\"Model initialization failed\")\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp3 = await process_all_batches_exp3(exp3_df, gemini_llm)\n",
    "display(results_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_original_df(exp3_excel, results_exp3, 'gemini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
